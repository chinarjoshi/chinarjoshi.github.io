---
layout: post
title: Regulators can dull, but never declaw GenAI due to Moore’s law
permalink: /regulation/
category: regulation
type: problem
---

“AI” has been the Wall Street Journal’s darling buzzword of the past year. I’m not really sure why I still read WSJ considering my home page has degenerated into opinion pieces calling the president an incorrigible senior and revealing the “Top 10 ways to retire at 63!” But if there’s any one topic their technology section loves to cover, it’s how FAANG/MAMAA/Magnificent 7 will break the world selling their “AIs” to every business imaginable, or how Elon Musk claims those “AIs” will lead to our doom. I can practically feel the business people salivating every time they hear of a new pseudo-profitable technology. But as I clean the spit off my screen, I realize it’s time to revisit whether we should actually ready our pitchforks and torches. There's a clear divide between people who understand the theory behind the technique, and people who invest a jaw dropping amount of money into the former. This is unfortunate, because the theory will be very useful knowledge when the consequences start popping up like whack-a-moles.

Even Google’s CEO unironically proclaimed that “AI will be more impactful to humanity than fire or electricity.” How convenient for someone with 227K shares in a company that claims itself at the frontier of AI to chum the investor tank… Fire provided light and warmth, a lethal weapon against lions, and newly digestible foods. Electricity amplified human effort to the limit of energy, increasing quality of life for everyone. These two inventions were game changers for the trajectory of humanity. In comparison, generative AI seems like a neat party trick, a proof of concept for something bigger. Yet, I invite you to read the lyrics of the Gershwin classic, [“They All Laughed.”](https://genius.com/George-gershwin-they-all-laughed-lyrics) Generative AI amplifies human communication to the limit of *creativity*, a truly potent but double edged sword. It will certainly push content moderation past its limits, ruin the creativity and dignity of countless people, even destroy the integrity of information itself, and there’s nothing the world can do about it except stop making better computers. Let me explain.

# Part 1: Generative AI is still just approximation

To understand the potential of the technology and why we should be careful, we have to attack it from different perspectives. Plus, I feel like there are many misconceptions surrounding AI and ML propagated both inside and outside the community. Like I mentioned, the media is incredibly bullish yet fearful of its future, predicting everything from a work-free utopia to a robot-dominated Orwellian dystopia. Even practitioners on Reddit and Stack Exchange frequently confuse the purpose and theory behind the methods. I once read someone on Reddit saying linear regression fits an arbitrary function to classify data. Both linear and regression are literally in the name… But the field is really just a diverse kitchen sink of problems. Each domain’s textbook provides a different perspective on those problems, but consolidating them all together yields the bigger picture. A cognitive psychology textbook explains the brain’s approach, a statistics textbook explains hypothesis testing and numerical methods, a deep learning textbook explains teaching neural networks, while a philosophy textbook explains the pursuit of truth. They all tell a different side of the same tale of approximation, but they’re split across different years of education and have their own nomenclature. Thus, let’s connect all the important concepts by breaking down large ideas to fill any gaps.

## The cognitive psychology approach

Artificial intelligence is the expression of statistics and logic through algorithms to make machines that exhibit intelligent characteristics. This definition is intentionally vague, because the sheer notion of “intelligence” is ethereal. There are multiple fields of psychology and science dedicated to understanding this concept. Let’s start with the cognitive psychology approach. The most applicable way I can think to consolidate them is “combining prior knowledge with hints from the input to solve problems.” This is the combination of top-down and bottom-up processing, the two primary approaches your brain uses for sensation and perception. Of course, achieving this level of processing requires:

* Memory, to store past stimuli in both a short-term working directory and a long-term knowledge bank, with mechanisms to retrieve and to forget
* Learning, to adapt through conditioning and observation to populate that knowledge bank
* Problem solving, to overcome obstacles in situations through pattern recognition and critical thinking
* Decision making, to react to situations appropriately through heuristics and to consolidate complex thought into an output.

But there is a good reason for this complex set of primitives. They allow us to reason through a diverse set of situations using a limited toolset, meaning they’re composable. Consider the problem of finding the shortest path between two points in a maze. Compare two strategies to pick the next point (aka vertex) to visit.

1. Always choose the next vertex closest to the start (Dijkstra/UCS). 
2. Always choose the next vertex closest to both the start and end-goal (A*).

Both approaches implicitly use hints from the input through the triangle inequality, that AB + BC > AC if B is not along the shortest path. Yet the first one isn’t considered intelligent because it chooses the best action only for the short term, without leveraging any prior knowledge to get to the answer faster. Meanwhile, the second one estimates the distance to the end-goal using a heuristic, and minimizes that distance quickly yet cheaply. A* is a small modification over Dijkstra’s, yet the inclusion of intelligent decision making helps it find the optimal solution much faster.

<img src="/assets/regulation/search.png" alt="Uniform cost vs A* search"/>

**Figure 1**. Uniform cost search *(left)* expands significantly more vertices than A* search *(right)* to find the shortest path.
  
A* is an example of a machine that behaves rationally, because it chooses the best option at every turn. But the field of graph search is very much a tiny subset of AI. Though artificial intelligence is quite an enormous discipline, two abstract properties of any solution are how much *action* it requires and how much *rationality* is desired. Therefore, you can chart the field using quadrants to reason about the nature of problems.
  
Accordingly, machines that *behave humanly* act in a way that emulates understanding of human emotions and social context. For example, social robots, video game NPCs, and ChatGPT try to imitate human behavior, even if they don’t internally function that way. Machines that *behave rationally* act in a way that maximizes utility, but whether that’s for the good of the user or society is open for interpretation. For example, self-driving cars, smart energy grids, and stock trading algorithms make the optimal decision at every turn. These problems all have something significant in common; the correct answer is influenced by a lot of variables and is therefore difficult to find, but a good-enough answer can be **approximated** using fewer variables.

<img src="/assets/regulation/quadrants.png" alt="The quadrants of AI"/>

**Figure 2**. Artificial intelligence is divided into 4 quadrants based on the desired amount of action and rationality.

## The philosophical approach

To understand the **theory of approximation**, we can turn to a thought experiment from 1990 that explains universal truth using particle physics, Laplace’s demon. Under the classical model of physics, if you know the current position and velocity of all particles in a system, then you can calculate the positions and velocities of any past and future time. This property applies from a simple two-particle system all the way to the entire observable universe, and holds because of conservation of energy. In other words, the state of the world changes in a predefined sequence (deterministic) and we can model and calculate these numbers (computable) given all positions and velocities at a single time (observable). A demon with unbounded knowledge could know such information about every particle, then it could answer any question with perfect accuracy. It could predict the weather 5 years from now, tell you every election result, plus how many of them will be manipulated. We can in fact model the demon as a function f(x1, x2, …, xN) -> y1, y2, …, yN, where each xi and yi represent particles. It takes in input one state of the universe and tells you the next. But the world is chaotic, and small phenomena can have huge consequences because of the butterfly effect. As soon as you start getting rid of variables, maybe leaving out some particles here and there, you lose potentially important interaction terms, and the model is no longer perfectly correct. What if that missing particle hits some atom in my brain in a certain way that compels me to buy Sony XM4s instead of XM5s? Then the demon is no longer perfectly accurate, but rather an approximation, a good-enough answer.

Laplace’s demon is a useful thought experiment because it shows the upper bound on the dimensionality of real world functions; they are discrete due to the atomicity of particles, and have finite domain and range because the speed of light is fixed. It’s clear that this logic applies to such functions of any dimensionality. In fact, it follows that every real world trend or phenomena is a deterministic function mapping f(x1, x2, …, xN) -> y1, y2, …, yM, where N,M <= particles in the universe. Now for convenience, let’s rewrite x1, x2, …, xN as a vector, X, an arrow in N-dimensional space, so now our functions are f(X) -> (Y). Our practical problem is we can’t possibly compute such a complex function using real computers. It’s too high dimensionality, and we must somehow reduce the number of variables to something we can handle easier. Let’s take a quick digression to probability to alleviate this problem. How do we know which variables we can safely overlook? We can only overlook some variables under an assumption of conditional independence. Once you know the values of the significant independent variables, then knowing the dependent variable doesn’t tell us anything about the “insignificant” or noisy independent variables. This is because there is little, maybe even no influence between the output and noise. But this also means when none of the variables are insignificant, then there is no way to approximate the function to any extent. A clear cut example of this “random” distribution is XOR: f(x, y, z) = x XOR y XOR z. If you know x = 1, then even if you know f(x, y, z) = 0, you haven’t learned anything about y or z. Even scaled up to millions of inputs, there is no way to approximate this function with less variables at all. In terms of information, these functions have maximum entropy with no redundancy.

Luckily, real world trends are often explained by significantly less variables than go into the whole equation. That’s a good thing because though tomorrow's weather may be a chaotic function of some quadrillion variables representing particles in the atmosphere, I still need to know whether to pack an umbrella. If the underlying distribution takes a set of parameters, X, we can partition them into two sets, (X’, E), where X’ are the k most important variables, and E are the rest. The important k variables capture most of the variance, or the way the distribution changes with inputs. The number of selected variables, k, depends on how much of this variance we wish to capture and how many dimensions we’re comfortable dealing with. This process is called feature selection, which we do implicitly when thinking about complex distributions <small>(feature is our measurement of an underlying variable)</small>. When I consider the likelihood of eating at Chick-fil-A, I’m probably just thinking about my hunger level and distance to the nearest one, not the restaurant’s Yelp rating or the amount of fuel in my car. This conditional independence between insignificant variables is notated as P(f(x) \| x, e) = P(f(x) \| x); we can comfortably forget about e. This property has significant implications, and indeed it is what enables approximation. There are functions in lower dimensions that still capture most of the variance of more complex functions. We call these lower-dimension functions approximators or models. The rest of the variables are more like noise: essential for a perfectly accurate answer, but can be overlooked to capture the heart of the problem. It’s just like the saying, “if you can’t explain it simply, you don’t understand it well enough.” This saying itself models the idea behind the theory of approximation. How cool is that!

We've established that **approximation is capturing maximum variance using minimum variables**. Back to our A* search example, the heuristic approximates the true distance from the goal state, which otherwise takes a long time to compute. If the true distance was a function, it might consider every vertex in the graph, while the heuristic would just consider the start and goal positions. In function notation, the true distance from s to t is dist(s, v1, v2, …, vN, t), where N = \|V\| - 2, while the heuristic is just h(s, t). Note that this notation is the same as f(x, y, z, …, w) multivariable calculus. If you’re used to programming, this might not seem like a big deal because those extra variables could just go unused. But think about the domain visually: the number of axes increases with every additional parameter, which causes the domain to grow exponentially. However, adding another parameter rarely “populates” the range at an equivalent rate, making the domain sparser and sparser with more parameters. This is fine if you know the closed-form definition of the function, like y = mx, but we’re often dealing with discrete distributions via observations. As the distribution’s domain gets sparser and sparser, you need exponentially more observations for the same density, or resolution of information. If we keep the same number of observations, our models will likely overfit the data points and just match them exactly instead of learning the true distribution, like pulling a string taut through a small set of holes. This problem is the **curse of dimensionality**.

<img src="/assets/regulation/curse.png" alt="The curse of dimensionality visualized"/>

**Figure 3**. Volume increases exponentially on the number of dimensions, causing the density of observations to decrease exponentially on the number of parameters.
  
The curse of dimensionality is the arch nemesis of approximation, giving us a huge incentive to keep our approximator function as simple as possible. Computer scientists fear exponential growth because we like both letting the domain be unbounded, and evaluating our functions in a reasonable time. Exponential functions do not allow these two properties to coexist because their growth accelerates, or increases multiplicatively, whereas polynomials increase additively. That’s why the A* heuristic function h ideally doesn’t take any intermediate points as parameters. But this heuristic requires some strong assumptions of dist, which is that s and t are really good indicators of the true distance. What do you do if the true distribution is too complex to know such a trick about? What if all you have are some (X, y) pairs at specific points? One incredibly successful method to find this good-enough answer using a set of observations is to tune some simple function to fit a more complex function. This statistical technique is called **supervised learning**. The enormous success of supervised learning has caused solutions from the behavioral half of AI to become integral to our daily lives.

## The optimization approach

<img src="/assets/regulation/mlexample.png" alt="The machine learning example scatterplot"/>

**Figure 4**. A set of observations *(purple)* initially approximated by the hypothesis parabola, y = x^2 + x + 1 *(blue)*.

For example, let’s say I have black-box function *f*. I don’t know what it is, but I can evaluate it at arbitrary points. So I did that to produce 30 observations, (0, 4.2), (1, 7.9), (2, 12.4), (3, 21.7), … (29, 1687.3). From initial thoughts, maybe from domain knowledge, this seems like a quadratic relationship between x and f(x). So let’s start with the general quadratic equation, g(x) = ax^2 + bx + c, and figure out the 3 coefficients, a, b, and c, that minimize the difference between our prediction and the given answer. There are an infinite number of real valued parabolas, and each one is a hypothesis that we think could explain the data. Thus we need to search for the hypothesis that explains the data the best. It doesn’t matter which hypothesis we start with, so let’s pick (1, 1, 1). Our initial predictions are thus (0, 1), (1, 3), (2, 7), …, (29, 871); quite far off, but we’re on the right track. Our goal is to find the values of coefficients that minimize the difference between my predicted g(x) and the f(x) from the observations. In other words, Our task is finding some parameters that model a distribution, given a set of (feature, class) observations. In this case, we have 1 feature, x, and 1 class, y. The number of features is the dimensionality of the dataset, and the number of observations is the size. By running supervised learning on our model given the observations, we can figure out that the underlying distribution is y = 2x^2 + 4 with some random noise. That means the best hypothesis was (2, 0, 4). Note that “evaluating a black-box function” is exactly what surveys, real-world measurements, simulations, and experiments fundamentally are, and fitting a simpler function to those observations is machine learning.

But there are limits to this approach depending on what kind of function we use to approximate. And we can precisely quantify this complexity-accuracy trade off using a beautiful tool we all forgot about from calculus: the **Taylor series**. It uses a power series polynomial to approximate any infinitely-derivable function centered around a point. But recall that a polynomial of limited degree can only approximate a higher order function for a specific interval before it diverges. Even with unlimited degree, most functions have a finite range of approximation. If the power series (approximator) converges with infinite terms, then we can directly calculate this radius of convergence using a ratio test. The idea behind Taylor series elegantly transfers to nonlinear approximators too: Fourier series globally approximate using a composition of sin and cosine, and wavelet transforms approximate using lots of small complex functions (like convolutional kernels). The key takeaways from these algorithms is that by using a more complex approximator, we increase both the accuracy and range of convergence, but nonetheless there is almost always a quantifiable limit.

<img src="/assets/regulation/taylor.gif" alt="Taylor series animation"/>

**Figure 5**. Higher degree polynomials can approximate y = sin(x) more closely than lower degree.

This is a fundamental headache for machine learning practitioners. Not only does the curse of dimensionality compel us to use simple models, but even if we were to use infinitely complex models, we still couldn’t perfectly approximate functions over their whole domain. Machine learning experienced a long winter from 1987-1993 when researchers believed we couldn’t approximate useful enough real-world functions because they were too complex for our models and computers. But the computer engineers were cooking up better computers superlinearly, and the pipe dream quickly became feasible. We still needed better models than polynomials to make the most of the abundant compute and datasets. They came up with some clever techniques like progressively partitioning your inputs to reduce entropy (decision trees), and finding the high-dimensional plane that maximizes the distance between the closest points of different classes (support vector machines). They even composed many of these models in parallel (bagging) and series (boosting), because combining many simpler models stochastically makes the most of our limited data.

But one architecture in particular proved to be versatile to a whole new extent. It combined linear regression with a nonlinear function sandwiched between each line. Try to visualize it in your head: the composition of two lines is just another line. The composition of 20 lines is still just a line. But add some non-linearity between each function, and it can take literally any shape over any range. As it turns out, this idea was the recipe to the most robust approximating function, the **neural network**. There’s a fundamental difference between neural networks and other models: a sufficiently complex neural network can approximate any real valued function over its entire domain. This technique bypasses the limit of polynomials explained by Taylor series, and of sinusoidal functions explained by Fourier series. Indeed, researchers formulated elegant proofs for the approximative capabilities of this architecture, called universal approximation theorems. Two theorems in particular gave a formal justification for the field of deep learning:

1. Any feedforward neural network (DAG) with sufficient width can approximate any continuous function for any sized interval
2. Any recurrent neural network (directed cyclic) with sufficient width can approximate any dynamical system for any set of states

We’ve talked about models a lot so far, while assuming we know the likeliest hypothesis (set of parameters) to expose the theoretical potential of each function. But how do we actually find those parameters, especially given our dataset size and compute limitations? By investigating Taylor series and universal approximation theorems, we’ve been operating under the decision problem, “is there a hypothesis with at most e error?” We need to reframe and solve the optimization problem, “what is the hypothesis with the lowest possible error?” Now this is a problem we can solve efficiently with algorithms like gradient descent, rather than with exhaustive search. Let’s notate the phrase “hypothesis with lowest possible error” as max h in H (P(h \| E), meaning the **hypothesis that best explains the data**. By definition of conditional probability, note that P(h \| E) = P(h, E) / P(E), which translates to “how likely is h true in the worlds where E is also true.” Note by the same property that the inverse is also true, that P(E \| h) = P(h, E) / P(h). Both of these equations can help us learn by capturing different probabilities about hypotheses and data, so let's combine them into one equation by their common term: P(h \| E) = P(E \| h) P(h) / P(E). This formula should be ringing bells at this point, because it's Bayes’s rule!

## The Bayesian statistics approach
  
<img src="/assets/regulation/bayes.png" alt="Bayes's Rule"/>

**Figure 6**. A prior belief can incorporate new information using a conditional probability.

Semantic definitions of these 4 terms helps intuit through the implications. Note that I’m substituting A with h for hypothesis, and B with E for evidence.
* Prior: How much did we believe this hypothesis before seeing the new evidence?
* Marginal: How likely is seeing this evidence across all possible hypotheses?
* Likelihood: How likely is seeing this evidence under this hypothesis?
* Posterior: How much do we believe this hypothesis in light of the new evidence?

Our objective is to explicitly **model the posterior, P(h \| E)**, and Bayes’s rule gives us the probabilistic machinery to calculate this. Starting with our current belief of this hypothesis, we incorporate the newly seen data through the ratio of the likelihood to the marginal likelihood. If this ratio is greater than 1, that means you’re likelier to see this data whenever h is true than under all possible hypotheses. Bayes’s rule is fundamentally an update step, one we take successively in order to learn. Our practical problem is that the likelihood and marginal are really difficult to model directly; how do you even answer the question “how likely is the air pressure to be 1.45atm?” Modeling the marginal is hard because it requires integrating over all possible hypotheses, and the number of hypotheses is exponential on the number of parameters. Modeling the likelihood is hard because it requires data that encompasses all variables at play; if you leave out significant latent variables, then the distribution is off even when you know when h is true. That’s why we implicitly model these two distributions together via the observation set. It’s less powerful because you can’t extract the marginal or likelihood, but combining them together is significantly easier to learn. Thus, our traditional supervised-learning approach to regression and classification problems is explicitly modeling the posterior distribution, allowing us to discriminate between different hypotheses to find the likeliest one given data.

The optimization approach we use to find P(h \| E) is to formulate error as a function, L, and find the root of dL/dW with the minimum value of L. Since the hypothesis h is a set of parameters, W, dL/dW tells you how much error changes for each unit change of parameters. Recall that the roots of dL/dW correspond to maxima or minima, and in this case we’re interested in minima: the smaller the error, the better that hypothesis explains the data. But formulating an equation offline that solves this directly at once, a closed-form solution, is another victim of the curse of dimensionality. Indeed, out of all the convex and differentiable optimization functions there are, humanity only knows of an algorithm to find the best line: least squares linear regression. Instead, we can learn online and **minimize the error function iteratively**, updating weights in the direction of the partial derivatives to guide our path. If L(X) and dL/dw1(X) have opposite signs, then updating w1 in the same direction as the gradient will minimize loss. Consequently, if L(X) and dL/dw1(X) have the same sign, then moving in the opposite direction of the gradient will minimize loss. Repeating this process for all partial derivatives of parameters tells you what direction to tweak every parameter to explain the data the best! The magnitude of update is less consequential than the direction, but nonetheless the best step size naturally scales with the slope; if you’re on a steep hill, you can take a bigger step with less fear of missing the bottom than on flatter ground. Even when using an error function that includes extra terms to avoid overfitting (like regularization), the derivative remains the same given you know h’. Thus, learning P(h \| E) is straightforward when h represents an elementary function, but what if the model is a composition of functions? Then you have to use the chain rule to find the derivative of the error function. And recall that neural networks are just a composition of elementary functions, so let’s explore that example.

## The deep learning approach

Taking the derivative of a neural network is a fascinating and strangely intuitive inductive process. For context, the network is a function f(X, L1, L2, …, Lk), where X is the input, each Li is a set of neurons whose input is the aggregated output of Li-1, and the network returns the output of Lk. Each neuron is a function g(X, W, a), where X is a k length vector of inputs, W is a k + 1 length vector of weights, a is a nonlinear function, and the neuron returns a(x dot w). But we only know what the output for layer Lk should be, because it’s the label. We can determine the error of Lk and optimize those neurons’ weights with the gradient descent algorithm described above. But this doesn’t help us refine the weights in L1 to Li-1. But recall that the input of Lk is the output of Lk-1, and the input of Lk-1 is the output of Lk-2, and this repeats inductively until you reach the original input, X. This implies that each neuron is responsible for some fraction of the next neuron’s error. And this fraction is satisfyingly simple to calculate: it’s the next neuron’s error multiplied by the corresponding weight for the previous neuron! After all, the weight represents exactly how much an input contributes to the final output. This weighted value is called the upstream gradient and is passed down to the previous layer, and for each neuron, the sum across the next layer represents how much that downstream neuron contributed to the final error. Now in order to calculate how much each individual weight contributed to this neuron’s total error contribution, we take the partial derivative dN/dW1 using chain rule. Multiply this local gradient by the upstream gradient, and we’ve just obtained the dL/dWi that gradient descent needs! Lastly, to pass the error downstream to the previous layer, we multiply this value by the corresponding weights like before. The process of using the partial derivatives with respect to weights to minimize the network’s error is called **backpropagation**, and is the fundamental algorithm of deep learning. This algorithm applies to RNNs as well because they can be unrolled into a DAG, producing the familiar feedforward network.

## Finally, generative AI

It’s time for the paradigm shift I mentioned at the start of the article, from discriminative to generative approximators. Luckily, the principles still follow Bayes’s rule, just with a different objective: **model the joint probability, P(E, h)**. This enables us to generate likely (feature, class) pairs that mimic those in the training set. But modeling joint probabilities is exponentially harder than modeling conditional probabilities like P(h \| E) because of the increase in dimensionality and complex interactions between these new dimensions. But recall from the definition of conditional probability that P(E, h) = P(h \| E)P(E) = P(E \| h)P(h). We already know P(h), our prior belief, and we know how to calculate P(h \| E) through our discriminative methods from above. So generative models can take the shortcut by explicitly modeling the marginal OR likelihood distributions. Although the marginal is usually more complex than the likelihood because it involves integrating over all possible hypotheses, our choice will depend on how the learning problem is set up. For practicality, we have to use some trick to cope with this computationally difficult task. Now we have a well defined task for generative AI: explicitly model the prior and likelihood, or the posterior and marginal likelihood!

**Naive Bayes** models are the simplest generative approximators. They assume that the features in E are conditionally independent given the hypothesis h, which simplifies the computation of P(E \| h) immensely. Under this assumption, P(E \| h) can be expressed as the product of individual probabilities for each feature, because P(Ei \| Ej,h) = P(Ei \| h). This makes it feasible to handle high-dimensional data spaces without dealing with the curse of dimensionality directly. This method effectively utilizes the prior P(h) and the likelihood P(Ei∣h) for each feature Ei​ to compute the joint probability P(E,h). While the conditional independence assumption may not hold true in all cases, it allows Naive Bayes to perform remarkably well in many practical scenarios, especially when the dimensionality of the feature space is large.

**Auto-regressive transformers** are currently the most complex and robust generative approximators, but they use a trick by modeling sequences. Their key insight is to weigh the importance of each element of the input sequence with every other element at once (self-attention mechanism), which both enables long run dependencies and makes the model massively parallelizable. Transformers are a combination of self-attention mechanisms with feedforward networks, and still learn through gradient descent since self-attention is a differentiable linear function. However, I mentioned that gradient descent is used to learn P(h \| E) in a supervised learning context, but generation requires modeling P(E, h), so what gives? Well, auto-regression blurs the line between h and E, and they learn the marginal and posterior simultaneously by decomposing E into a sequence of conditional probabilities. Remember how ChatGPT has to generate one token at a time to “reconstruct” the whole sequence? In this learning configuration, the hypothesis at each step is the next word, and the evidence is every word before it. There is still an (X, y) pair at every step, but y is simply the last actual token of X. So iteratively sampling from P(xn \| x1, x2, …, xn-1) gives you the likely hypothesis, and together they construct the marginal, P(E). Modeling sequences is the clever shortcut that auto-regressive models use to be generative.

<img src="/assets/regulation/marginal.png" alt="The marginal expressed as a sequence"/>

**Figure 7**. The marginal likelihood can be decomposed into a sequence of auto-regressive conditional probabilities.

## Morals of the story

This top-down story consolidates the cognitive psychology, philosophy, optimization, Bayesian statistics, and finally deep learning perspectives on AI. You should understand that ChatGPT works under the same conceptual foundations as A* search. And I think there are two conflicting morals to this story. First, keep an open mind when making assumptions about the world. If someone else’s assumption is easier to explain while still explaining the data, then it’s probably more accurate than yours due to the curse of dimensionality. Humans are merely agents in a partially observable environment, and as such our observations are finite and biased. Choosing a simpler hypothesis allows you to make the most of your finite observations by learning the underlying trend instead of the points, just like believing in coincidences over ghosts, or even the line (2, 0, 4) over the line (10, -3, 13, 7). This simplicity-bias is known colloquially as **Occam’s razor**. Practically, it guides us to think simply and try out linear regression before neural networks. However, this story was just the theoretical side of the coin; back on Earth, we are practically limited by the speed and memory of computers and the quality and quantity of observations. But I will show that these limitations are lifting at an exponential rate, ultimately allowing us to reach said theoretical limits of our approximators.

Finally, take that first moral with a huge grain of salt. By definition, machine learning is a dumb optimization algorithm that finds correlation between evidence and hypotheses. The simplest hypothesis that explains the data is usually a correlative relationship, not a causative one. This is precisely why models predict that being black or Muslim makes you more dangerous than being white, and being a woman makes you less suited for male-dominated careers; there are certainly such correlative relationships in our datasets, but relationships do not attempt to explain the true underlying distribution. Balancing the preference for simplicity with the preference for causation birthed the field of **AI Ethics & Safety**. And as people making models that can have a significant impact on the consumer’s life, it is our responsibility to stay vigilant of such dangerous correlations and stay skeptical of very high accuracy models. But I didn’t agree with this sentiment for a long time for the same reason I would take money from a lost wallet: if I don’t do it, someone else will. Caring about AI safety is at odds with accuracy, which is lethal in such a competitive and fast paced market. Free market forces should surely prefer companies that move fast and break things, right? But safety has tangible financial value, from a lucrative avenue for marketing to avoidance of potential lawsuits down the line. There is a third axis for the utility of models, which is alignment with good human ethos vs discriminatory bias. But whether companies actually do this hard work before plastering *“we care about AI safety!”* on their websites is another story…
